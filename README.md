# Zarr_Benchmarking
This Research Project report encapsulates a detailed overview of the performance evaluation of the Zarr library that is used to store multi dimensional arrays or tensors. The advantage of using Zarr is that it can store the datasets into several chunks which can be user defined and those compressed chunks can be accessed while reading the data. Consequently there are two main advantages to it. Firstly, storing compressed chunks requires less space and also accessing data from dataset does not require the whole dataset to be loaded, instead it requires the particular chunk and only that chunk can be loaded resulting in faster access speeds. In this report I have configured different parameters(eg. Compressors, blocksize, shuffle) while creating the data sets and observed the impact on accessing the data. Also I evaluated the native Zarr queries with different configurations and evaluated their performance. I have used Read Time, Average CPU usage and Disk IO count to evaluate the performance. 
Zarr is a storage file format that is specified by .zarr extension. It is used to store large dataset using multidimensional arrays. It is useful to store data from Life Science/climate forecasts domain which contains data upto 7 dimensions. Zarr provides a very intelligent and efficient way to store data in memory by the method of chunks. This chunks can be defined by the user at the time of creation or Zarr can by itself define the chunks based on the dataset structure. This chunks are stored individually in memory in a compressed way. And when a particular data is accessed, Zarr loads up the chunk where the data may reside instead of loading up the whole dataset into the memory which improves the access speed as well.
There are multiple compressors available in for the implementation in Zarr. It uses a library called \textit{numcodecs} which contains various compressor classes such as Blosc, LZ4, Zstd, Zlib, LZMA, GZip and various others. For this project, blosc framework \cite{zeyen2017cosmological} is selected for the benchmarking since it is the default compressor provided in the Zarr library. Now different algorithms are used inside blosc for the benchmarking evaluation such as lz4, lz4hc, zlib etc. Now all these algorithms requires certain parameters for creating a blosc compressor object.
Chunking can significantly reduce the access time and increase the efficiency for storing the data in memory. But to attain an optimal chunking strategy is dependent of data and there is no generic optimal strategy for Zarr. Although there has been a literature available which discussed about some optimal chunking strategy using geometrical programming and steep descent optimization method. There is a way to configure the chunks while creating a Zarr array. Now it depends on the data access patterns to set the chunking strategies. The chunking can be done on various dimensions. As per the Zarr documentation, 1 Megabyte uncompressed size seems to provide better performance. So if there is a data of 2 dimensions and there is only need to access the first dimension then only the second dimension can be chunked. 
